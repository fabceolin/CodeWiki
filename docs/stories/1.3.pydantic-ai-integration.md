# Story 1.3: pydantic-ai Integration

## Status

Draft

## Story

**As a** CodeWiki developer,
**I want** a `ShellModel` class compatible with pydantic-ai,
**so that** the existing `AgentOrchestrator` can use shell-based LLM calls without code changes.

## Dependencies

- **Story 1.1:** Shell LLM Adapter (provides `ShellLLMAdapter` class)
- **Story 1.2:** Provider Configuration (provides `provider_type`, `shell_command`, `shell_args`, `shell_timeout` config fields)

## Acceptance Criteria

1. `ShellModel` class implements pydantic-ai model interface (`Model` protocol)
2. `ShellModel` uses `ShellLLMAdapter` from Story 1.1 internally
3. Factory function `create_main_model(config)` returns appropriate model based on `provider_type`
4. Factory function `create_fallback_models(config)` works with shell provider
5. `AgentOrchestrator` works with `ShellModel` without any code changes
6. `call_llm()` function in `llm_services.py` supports shell provider
7. End-to-end `codewiki generate` works with `--provider shell` configuration

## Limitations (v1 Scope)

**Tool Calling:** Shell provider does NOT support pydantic-ai tool calling in v1. The `claude` CLI has its own tool mechanism that is not compatible with pydantic-ai's `tool` decorator pattern.

**Impact:** CodeWiki agents that use `read_code_components_tool`, `str_replace_editor_tool`, and `generate_sub_module_documentation_tool` will NOT work with shell provider in v1.

**Workaround for v1:** Shell provider can be used for simpler LLM calls (e.g., `call_llm()` function for clustering). Full agent support requires API provider.

**Future:** Tool calling support can be added in a future story by implementing a tool-result loop that parses claude CLI output and re-invokes with tool results.

## Tasks / Subtasks

- [ ] Task 1: Research pydantic-ai Model interface (AC: 1)
  - [ ] Analyze `pydantic_ai.models.openai.OpenAIModel` implementation
  - [ ] Identify required methods: `request()`, `name` property
  - [ ] Understand `ModelResponse` return type
  - [ ] Document interface contract

- [ ] Task 2: Implement `ShellModel` class (AC: 1, 2)
  - [ ] Create `shell_model.py` in `src/be/`
  - [ ] Implement `Model` protocol from pydantic-ai
  - [ ] Implement `async def request()` method
  - [ ] Integrate with `ShellLLMAdapter` from Story 1.1
  - [ ] Handle model name/settings mapping
  - [ ] Implement `name` property

- [ ] Task 3: Update factory functions (AC: 3, 4, 6)
  - [ ] Modify `create_main_model(config)` to check `provider_type`
  - [ ] Return `ShellModel` when provider is `shell`
  - [ ] Return `OpenAIModel` when provider is `api` (existing behavior)
  - [ ] Modify `create_fallback_models(config)` for shell provider
  - [ ] Update `call_llm()` to support shell provider

- [ ] Task 4: Integration testing (AC: 5, 6, 7)
  - [ ] Test `call_llm()` function with shell provider
  - [ ] Test `AgentOrchestrator` initialization with `ShellModel` (will fail on tool calls - expected)
  - [ ] Document limitation: full `codewiki generate` requires API provider due to tool calling
  - [ ] Add graceful error message when shell provider used with tool-requiring agents

## Dev Notes

### Files to Create/Modify

```
codewiki/src/be/shell_model.py     # NEW: ShellModel class
codewiki/src/be/shell_adapter.py   # From Story 1.1
codewiki/src/be/llm_services.py    # MODIFY: Update factory functions
```

### Current Architecture (llm_services.py)

```python
# Current implementation
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.models.fallback import FallbackModel

def create_main_model(config: Config) -> OpenAIModel:
    return OpenAIModel(
        model_name=config.main_model,
        provider=OpenAIProvider(
            base_url=config.llm_base_url,
            api_key=config.llm_api_key
        ),
        settings=OpenAIModelSettings(...)
    )

def create_fallback_models(config: Config) -> FallbackModel:
    main = create_main_model(config)
    fallback = create_fallback_model(config)
    return FallbackModel(main, fallback)
```

### Target Architecture

```python
from typing import Union
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.models.fallback import FallbackModel
from codewiki.src.be.shell_model import ShellModel

ModelType = Union[OpenAIModel, ShellModel]

def create_main_model(config: Config) -> ModelType:
    if config.provider_type == "shell":
        return ShellModel(
            command=config.shell_command,
            args=config.shell_args,
            timeout=config.shell_timeout
        )
    return OpenAIModel(...)  # existing implementation

def create_fallback_models(config: Config) -> Union[FallbackModel, ShellModel]:
    if config.provider_type == "shell":
        # Shell provider doesn't have fallback - it's one CLI
        return create_main_model(config)
    # Existing fallback logic for API
    main = create_main_model(config)
    fallback = create_fallback_model(config)
    return FallbackModel(main, fallback)
```

### pydantic-ai Model Protocol

Based on pydantic-ai source, the `Model` protocol requires:

```python
from typing import Protocol, AsyncIterator
from pydantic_ai.messages import ModelMessage, ModelResponse

class Model(Protocol):
    @property
    def name(self) -> str:
        """Return model name for logging/identification."""
        ...

    async def request(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None = None,
    ) -> ModelResponse:
        """Make a request to the model."""
        ...

    # Optional: for streaming support
    async def request_stream(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None = None,
    ) -> AsyncIterator[StreamedResponse]:
        """Stream a response from the model."""
        ...
```

### ShellModel Implementation Skeleton

```python
"""Shell-based LLM model for pydantic-ai."""
from typing import Any
from dataclasses import dataclass

from pydantic_ai.messages import (
    ModelMessage,
    ModelResponse,
    TextPart,
    ModelResponsePart,
)
from pydantic_ai.models import Model, ModelSettings

from codewiki.src.be.shell_adapter import ShellLLMAdapter


@dataclass
class ShellModelSettings:
    """Settings for ShellModel."""
    timeout: int = 300


class ShellModel(Model):
    """pydantic-ai compatible model using shell CLI execution."""

    def __init__(
        self,
        command: str = "claude",
        args: list[str] | None = None,
        timeout: int = 300,
    ):
        self._command = command
        self._args = args or ["-p", "{prompt}", "--dangerously-skip-permissions"]
        self._timeout = timeout
        self._adapter = ShellLLMAdapter(
            command=command,
            args=self._args,
            timeout=timeout
        )

    @property
    def name(self) -> str:
        """Return model identifier."""
        return f"shell:{self._command}"

    async def request(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None = None,
    ) -> ModelResponse:
        """Execute shell command and return response."""
        # Convert pydantic-ai messages to simple dict format
        formatted_messages = self._convert_messages(messages)

        # Get timeout from settings if provided
        timeout = self._timeout
        if model_settings and hasattr(model_settings, 'timeout'):
            timeout = model_settings.timeout

        # Execute via adapter
        response_text = await self._adapter.execute(
            messages=formatted_messages,
            timeout=timeout
        )

        # Build ModelResponse
        return ModelResponse(
            parts=[TextPart(content=response_text)],
            usage=None,  # Shell doesn't provide token usage
        )

    def _convert_messages(self, messages: list[ModelMessage]) -> list[dict]:
        """Convert pydantic-ai messages to simple dict format.

        Note: ToolCallPart and ToolReturnPart are ignored in v1.
        Shell provider does not support tool calling.
        """
        result = []
        for msg in messages:
            if hasattr(msg, 'parts'):
                for part in msg.parts:
                    # Only process TextPart, ignore ToolCallPart/ToolReturnPart
                    if hasattr(part, 'content') and isinstance(part.content, str):
                        result.append({
                            "role": getattr(msg, 'role', 'user'),
                            "content": part.content
                        })
        return result
```

### Integration Point: agent_orchestrator.py

The `AgentOrchestrator` (line 64) creates models via:

```python
self.fallback_models = create_fallback_models(config)
```

And uses them in `Agent`:

```python
agent = Agent(
    self.fallback_models,  # This receives our ShellModel
    name=module_name,
    deps_type=CodeWikiDeps,
    tools=[...],
    system_prompt=format_system_prompt(...)
)
```

**No changes needed to AgentOrchestrator** - it just needs the factory to return a compatible model.

### Tool Calling Limitation (v1)

pydantic-ai Agents use tool calling via the `@tool` decorator. The `claude` CLI has its own MCP-based tool mechanism that is incompatible with pydantic-ai's protocol.

**v1 Scope:**
- Shell provider works for `call_llm()` direct calls (used by clustering)
- Shell provider does NOT work for `AgentOrchestrator` which requires tools
- Attempting to use shell provider with agents will result in errors when tools are invoked

**Implementation:**
- `_convert_messages()` ignores `ToolCallPart` and `ToolReturnPart`
- Factory should log warning when shell provider is selected: "Shell provider does not support tool calling. Use API provider for full agent support."

**Future Enhancement (Story 1.4+):**
Tool calling could be implemented by:
1. Parsing claude CLI output for tool call JSON
2. Executing tools locally
3. Re-invoking claude with tool results
This requires significant additional work and is out of scope for v1.

### Testing

- Test file: `tests/test_shell_model.py`
- Integration test: `tests/integration/test_shell_provider.py`

```python
@pytest.mark.asyncio
async def test_shell_model_request():
    with patch.object(ShellLLMAdapter, 'execute') as mock_execute:
        mock_execute.return_value = "Hello from Claude CLI"

        model = ShellModel(command="claude")
        messages = [
            ModelMessage(parts=[TextPart(content="Hello")], role="user")
        ]

        response = await model.request(messages)

        assert response.parts[0].content == "Hello from Claude CLI"
        mock_execute.assert_called_once()
```

### End-to-End Test

```bash
# Configure shell provider
codewiki config set --provider shell

# Run generation
codewiki generate --verbose

# Verify documentation was generated
ls docs/
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-20 | 0.1 | Initial story creation | Sarah (PO) |
| 2025-01-20 | 0.2 | Added detailed pydantic-ai interface analysis and implementation skeleton | Sarah (PO) |
| 2025-01-20 | 0.3 | Applied validation fixes: added Dependencies section, documented tool calling limitation, updated Task 4, improved _convert_messages | Sarah (PO) |
